---
title: Recursive Partitioning <br> for Heterogeneous Causal Effects
subtitle: Susan Athey and Guido Imbens(2016)
author: Wooyong Park, Joonwoo Shin, Jeongwoo Yang, Minseo Seok
format: coeos-revealjs
---


# Data-driven HCE analysis based on tree models
Don't stress. We are going to start from scratch.


## Causal Inference with binary treatment

Research Question:

Does a smile in your online profile help your online micro-borrowing?(Athey et al, 2022 <small>NBER WP</small>)

::: {.fragment .custom .fadein-gray} 
* $W_i \in \{0,1\}$ : Your treatment status

<div style="margin-bottom: 0.8em;"></div>

Under SUTVA(*Stable Unit Treatment Values Assumption*), 

* $Y_i(1)$ : individual $i$'s potential outcome if he/she smiled
* $Y_i(0)$ : individual $i$'s potential outcome if he/she did not

<div style="margin-bottom: 0.8em;"></div>

Your **treatment effect** is the difference between the two:
$$
\tau_i = Y_i(1) - Y_i(0)
$$
:::

## Missing Counterfactuals and ATE

#### What we see:
$$
Y_i = W_iY_i(1) + (1-W_i)Y_i(0)
$$

Since we cannot observe both situations,
we usually rely on ATE:

$$
\tau = \mathbb{E}\bigl[Y_i(1)-Y_i(0)\bigr]
$$



If


&nbsp; &nbsp; &nbsp; &nbsp; 1. the treatment is randomized

&nbsp; &nbsp; &nbsp; &nbsp; 2. the treatment is uncorrelated with unobserved characteristics

&nbsp; &nbsp; &nbsp; &nbsp; 3. we have an IV

we can unbiasedly estimate the ATE.



## Heterogeneous Treatment Effects

### Limitations of ATE

Sometimes, the ATE is insufficient. 

Unfortunately, some people's smiles might not be as alluring as others'. $\rightarrow$ HTE

<br>

:::{.fragment }

#### Conditional ATEs

<div style="margin-bottom: 0.5em;"></div>

CATE tries to explain them within the data:


$$
\tau(X_i) = \mathbb{E}\bigl[Y_i(1) - Y_i(0)|X_i\bigr]
$$

::: {.fragment .custom .fadein-gray}

Athey and Imbens(2016): Tree-based model, a ML algorithm, can hint on how to choose $X_i$

:::


:::



## Key concepts in ML

In terms of prediction, OLS($\mathrm{y}=X'\beta +\varepsilon$) is not good enough.

::: {.fragment .custom .fadein-gray}


2. Bias-Variance tradeoff in Prediction
    * Let $y=f(x) +\varepsilon$ be the true DGP and $\hat{f}$ be our fitted model.
    * Then, for a new data point $x$,
    $$
    \mathbb{E}\bigl[\bigl(f(x)+\varepsilon-\hat{f}(x)\bigr)^2\bigr] = \mathbb{E}[f-\hat{f}]^2 + \mathbb{V}(\hat{f}) + \mathbb{V}(\varepsilon)
    $$
    
    * The unbiased predictor is not usually the minimum-error predictor, but most models/estimations including OLS focus on unbiasedness!

::: {.fragment .custom .fadein-gray}

"Learning" means that the data selects the right parameters/hyperparameters, not the researcher.

$\rightarrow$ can maybe help us set up models and hypotheses!

:::


:::

## Classification and Regression Trees(CART)

:::: {.panel-tabset}


### Trees (1)

* Trees recursively partition the covariate space so that MSE decreases each time we add a partition.

::::{.columns}

:::{.column width="70%"}

```{r}
#| echo: true
#| output: true
#| message: false

# See the full code at Grant Mcdermott's repository
library(rpart) 
library(parsnip)
library(tidyverse)
library(parttree)
set.seed(123) ## For consistent jitter

fit = rpart(Kyphosis ~ Start + Age, data = kyphosis)

ggplot(kyphosis, aes(x = Start, y = Age)) +
  geom_parttree(data = fit, alpha = 0.1, aes(fill = Kyphosis)) + # <-- key layer
  geom_point(aes(col = Kyphosis)) +
  labs(
    x = "No. of topmost vertebra operated on", y = "Patient age (months)",
    caption = "Note: Points denote observations. Shading denotes model predictions."
    ) +
  theme_minimal()
```

:::


:::{.column width="30%"}

:::

::::

### Trees (2)


* Trees recursively partition the covariate space so that MSE decreases each time we add a partition.

![Figure from Beaulac and Rosenthal(2019)](figures/decision_tree_illustration.png)

### Trees (3)

* Trees recursively partition the covariate space so that MSE decreases each time we add a partition.

1. Split the data into $S^{tr}$(training) and $S^{te}$(test).

2. Choose $X_i$ and cutoff $k$ and minimize MSE:

$$
\sum_{i \in L}(y_i-\overline{y}_L)^2 +\sum_{i \in R}(y_i-\overline{y}_R)^2
$$

3. Repeat this process of

4. $\hat{y}(\text{leaf}) = \overline{y}_{(\text{leaf})} \quad$ (i.e. sample mean)

5. Evaluate the prediction performance in $S^{te}$.

::::


## CART - Best predictor, but biased

:::: {.panel-tabset}

### Sample mean of each leaf

```{r}
#| echo: true
#| output: true
#| message: false

# Load required library
library(rpart)
library(ggplot2)
library(rpart.plot)

set.seed(123)

# simulate data and apply CART with two features
n <- 100

x1 <- runif(n, 0, 10)
x2 <- runif(n, -5, 5)
y <- 1 + 2 * x1 + 3 * x2 * as.numeric(x2>0) + rnorm(n, 0, 1)  # True relationship: y = 1 + 2 x1 + 3 x2*I(x2>0) + noise
    
# Fit regression tree
data <- data.frame(x1 = x1, x2 = x2, y = y)
tree_model <- rpart(y ~ x1 + x2, data = data, control = rpart.control(cp = 0.01))
    
# Pruning
best <- tree_model$cptable[which.min(tree_model$cptable[,"xerror"]),"CP"]
pruned_tree <- prune(tree_model, cp=best)

rpart.plot(pruned_tree, digits=3)
```


### Actual Sample Mean with Larger / independent samples

```{r}
#| echo: true
#| output: true
#| message: false

# Load required library
library(tidyverse)

# Make a data and compute the sample mean in each leaf
n <- 10000000

x1 <- runif(n, 0, 10)
x2 <- runif(n, -5, 5)
y <- 1 + 2 * x1 + 3 * x2 * as.numeric(x2>0) + rnorm(n, 0, 1)  # True relationship: y = 1 + 2 x1 + 3 x2*I(x2>0) + noise

data <- tibble(x1 = x1, x2 = x2, y = y) |> 
    mutate(leaf = case_when(
        x1<2.4 & x2<1.34 ~ "1",
        x1>2.4 & x2<1.34 & x1<4.16 ~ "2",
        x1<4.16 & x2>=1.34 ~ "3",
        x1<6.84 & x1>=4.16 & x2<3.61 ~ "4",
        x1<6.84 & x2>=3.61 ~ "5",
        x1>=6.84 & x2<0.237 ~ "6",
        x1>=6.84 & x2>=0.237 & x2<2.39 ~ "7",
        x1>=6.84 & x2>=2.39 ~ "8"
    ))

df <- data |> group_by(leaf) |> summarize(unbiased_mean = mean(y)) |> 
    mutate(tree_mean = c(4.35, 8.26, 11.6, 12.94, 19.85, 17.84, 22.15, 29.02))

ggplot(df, aes(as.numeric(leaf))) +
    geom_point(aes(y=unbiased_mean, color='ub')) +
    geom_point(aes(y=tree_mean, color = 'b')) +
    scale_x_continuous(labels = 1:8, breaks=1:8) +
    scale_color_manual(name =NULL,  values=c("ub" = "red", "b" = "blue"), labels = c("tree", "true DGP")) +
    labs(title = "CART mean vs true mean", x="leaf", y="mean") +
    theme_bw()

```

::::


# Honest Inference for Outcome Averages

## CART vs Honest

### CART

$$ 
\hat{Y_{i,c}}(\underbrace{S^{tr}}_{\text{training sample}},\underbrace{\pi_C(S^{tr})}_{\text{splitting for CART}})=\displaystyle\sum\limits_{l \in \pi_C(S^{tr})}\underbrace{\hat{\mu_l}(S^{tr},\pi_C(S^{tr}))}_{\text{within leaf estimate}}\mathbf{1}_{l}(x)
$$

### Honest

$$
\hat{Y_{i,H}}(\underbrace{{S^{\color{red} est}}}_{\text{estimation sample}},\underbrace{\pi_H(S^{tr})}_{\text{splitting for Honest}})=\displaystyle\sum\limits_{l \in \pi_H(S^{tr})}\hat{\mu_l}({S^{\color{red} est}},\pi_H(S^{tr}))\mathbf{1}_{l}(x)
$$

- where $\hat{\mu_l}(S,\Pi)=\frac{1}{\#(i \in S :X_i \in l)}\sum\limits_{i \in S: X_i \in l}Y_i$
- Honest means we are using two samples, one for splitting and one for within leaf estimating 

## Limitations of CART
- We cannot simply use CART to estimate HTE.
    + Potential bias in the leaf estimates
    + does not consider variance in tree splitting


 ::: {.fragment .custom .fadein-gray}
 
 Suppose the feature space is $\quad \mathbb{X} = \{L,R\}$

Recursive partitioning simply does the following:

\begin{align*}
    \pi(S)= \begin{cases}
    \quad \{ \{L,R \} \} & \text{if} \quad \bar{Y}_L -\bar{Y}_R \leq c \\
    \quad \{ \{L\}, \{R \} \} & \text{if} \quad \bar{Y}_L -\bar{Y}_R > c
    \end{cases}
\end{align*}

If we condition on $\bar{Y}_L -\bar{Y}_R >c$, we expect bias.

:::

## Limitations of CART
### Example


::::{.panel-tabset}


### Training 1

:::: { .columns}

::: { .column width="70%"}
```{r}
#| echo: true
#| output: true
#| message: false

# See the full code at Grant Mcdermott's repository
library(rpart) 
library(parsnip)
library(tidyverse)
library(parttree)

train1 <- kyphosis %>% head(50)
estimation <- kyphosis %>% tail(31)
set.seed(456) ## For consistent jitter
train2 <- kyphosis %>% sample_n(50)

fit = rpart(Kyphosis ~ Start + Age, data = train1)

ggplot(train1, aes(x = Start, y = Age)) +
  geom_parttree(data = fit, alpha = 0.1, aes(fill = Kyphosis)) + # <-- key layer
  geom_point(aes(col = Kyphosis)) +
  labs(
    x = "No. of topmost vertebra operated on", y = "Patient age (months)",
    caption = "Note: Points denote observations. Shading denotes model predictions."
    ) +
  theme_minimal()
```

:::

::: { .column width="30%"}

:::

::::

### Training 2

:::: { .columns}

::: { .column width="70%"}

```{r}
#| echo: true
#| output: true
#| message: false



fit = rpart(Kyphosis ~ Start + Age, data = train2)

ggplot(train2, aes(x = Start, y = Age)) +
  geom_parttree(data = fit, alpha = 0.1, aes(fill = Kyphosis)) + # <-- key layer
  geom_point(aes(col = Kyphosis)) +
  labs(
    x = "No. of topmost vertebra operated on", y = "Patient age (months)",
    caption = "Note: Points denote observations. Shading denotes model predictions."
    ) +
  theme_minimal()
```

:::

:::{ .column width="30%"}

:::

::::

### Estimation Set

:::: { .columns}

::: { .column width="70%"}

```{r}
#| echo: true
#| output: true
#| message: false


fit = rpart(Kyphosis ~ Start + Age, data = kyphosis)

ggplot(kyphosis, aes(x = Start, y = Age)) +
#   geom_parttree(data = fit, alpha = 0.1, aes(fill = Kyphosis)) + # <-- key layer
  geom_jitter(aes(col = Kyphosis)) +
  geom_segment(aes(x = 0, xend = 12.5, y = 35, yend = 35), linetype="dashed") +
  geom_vline(aes(xintercept=12.5), linetype="dashed") +
  labs(
    x = "No. of topmost vertebra operated on", y = "Patient age (months)",
    caption = "Note: Points denote observations. Shading denotes model predictions."
    ) +
  theme_minimal()
```

:::

:::{ .column width="30%"}

:::

::::

::::





## The Honest Criterion

\begin{align*}
        \text{MSE}_{\mu}(\underbrace{S^{te}}_\text{test sample},\underbrace{S^{est}}_\text{estimation sample},\Pi) &= \frac{1}{\#(S^{te})}\sum\limits_{i \in S^{te}}\left[(Y_i - \hat{\mu}(X_i;S^{est},\Pi))^2-Y_i^2\right]
\end{align*}

<div style="margin-bottom: 0.4em;"></div>

\begin{align*}
        \text{EMSE}_\mu &= \mathbb{E}_{S^{te},S^{est}}\left[\text{MSE}_\mu(S^{te},S^{est},\Pi)\right]
\end{align*}

<div style="margin-bottom: 0.4em;"></div>

Then, our goal is to find $\pi$ that would maximize the **Honest Criterion**:
\begin{equation*}
        -\mathbb{E}_{S^{te},S^{est},S^{tr}}\biggl[MSE_\mu\bigl(S^{te},S^{est},\pi(S^{tr})\bigr)\biggr]
\end{equation*}

- $S^{te}$ : We take expectation over the test set
- $S^{tr}$ : Tree is built out of the training set
- $S^{est}$ : $\hat{\mu}$ is computed in the estimation set


## The Honest Target
- Given $\Pi$, we can expand $EMSE_\mu(\Pi)$ :

\begin{align*}
-\text{EMSE}_\mu(\Pi) &= 
        -\mathbb{E}_{(Y_i,X_i),S^{est}}\left[(Y_i-\mu(X_i;\Pi))^2 - Y_i^2\right] \\ 
        &\quad-\mathbb{E}_{X_i,S^{est}}\left[(\hat{\mu}(X_i;S^{est},\Pi)-\mu(X_i;\Pi))^2\right]\\
        &\quad= \mathbb{E}_{X_i}\left[\mu^2(X_i;\Pi)\right] - \mathbb{E}_{S^{est},X_i}\left[\mathbb{V}(\hat{\mu}(X_i;S^{est},\Pi))\right]
\end{align*}

- How can we estimate each of these terms using $\quad S^{tr}\quad$ and $\quad N^{est}$?

## Honest Target: Estimation

$$
     -\text{EMSE}_\mu=\mathbb{E}_{X_i}\left[\mu^2(X_i;\Pi)\right] - \mathbb{E}_{S^{est},X_i}\biggl[\mathbb{V}\bigl(\hat{\mu}(X_i;S^{est},\Pi)\bigr)\biggr]
$$

::: {.panel-tabset}

### first term

$$
            \hat{\mathbb{E}}\left[\mu^2(x;\Pi)\right] = \hat{\mu}^2(x;S^{tr},\Pi)-\frac{S^2_{S^{tr}}(\mathcal{l}(x;\Pi))}{N^{tr}(\mathcal{l}(x;\Pi))}
$$



### second term

$$
            \hat{\mathbb{V}}(\hat{\mu}(x;S^{est},\Pi)) = \frac{S^2_{S^{\color{red} tr}}(l(x;\Pi))}{N^{est}(l(x;\Pi))}
$$

Assuming leaf shares between $S^{tr}$ and $S^{est}$ are approximately the same,

$$
            \hat{\mathbb{E}}\left[\mathbb{V}(\hat{\mu}(X_i;S^{est},\Pi))|i \in S^{te}\right] = \frac{1}{N^{est}}\sum\limits_{\mathcal{l} \in \Pi}S^2_{S^{tr}}(\mathcal{l})
$$

:::

## Honest Target: Estimation

$$
     -\text{EMSE}_\mu=\mathbb{E}_{X_i}\left[\mu^2(X_i;\Pi)\right] - \mathbb{E}_{S^{est},X_i}\biggl[\mathbb{V}\bigl(\hat{\mu}(X_i;S^{est},\Pi)\bigr)\biggr]
$$

The two terms combined, we obtain an unbiased estimator for honest target:
\begin{align*}
        \begin{aligned}
        -\widehat{\text{EMSE}_\mu}(S^{tr},N^{est},\Pi) =\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad 
        \\    \frac{1}{N^{tr}} \sum\limits_{i \in S^{tr}}\hat{\mu^2}(X_i;S^{tr},\Pi)-\left(\frac{1}{N^{tr}}+\frac{1}{N^{est}}\right)\cdot\sum\limits_{l \in \Pi}S^2_{S^{tr}}(l(x;\Pi))
        \end{aligned}
\end{align*}



## Comparison to CART

### CART Target

\begin{equation*}
            -\text{MSE}_{\mu}(S^{tr},S^{tr},\Pi) = \frac{1}{N^{tr}} \sum\limits_{i \in S^{tr}}\hat{\mu^2}(X_i;S^{tr},\Pi)
\end{equation*}


### Honest Target
\begin{equation*}
        \begin{aligned}
        -\widehat{\text{EMSE}_\mu}(S^{tr},N^{est},\Pi) =\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad 
        \\    \frac{1}{N^{tr}} \sum\limits_{i \in S^{tr}}\hat{\mu^2}(X_i;S^{tr},\Pi)-{\color{blue}\left(\frac{1}{N^{tr}}+\frac{1}{N^{est}}\right)\cdot\sum\limits_{l \in \Pi}S^2_{S^{tr}}(l(x;\Pi))}
        \end{aligned}
\end{equation*}

## Comparison to CART

### Pros and Cons of Honest
- Pro: 
    * Honest target not only removes potential bias in leaf estimates but also considers variance reduction in splitting.
    * enables statistical testing(valid confidence intervals)
- Con: smaller sample size, shallower tree, and less personalized predictions





# Honest Inference for Treatment Effects

## Honest Inference for Treatment Effects
- Population average outcome "in a leaf" and its estimator

\begin{align*}
            &\mu(w,x;\Pi) \equiv \mathbb{E}[Y_i(w)|X_i \in \mathcal{l}(x;\Pi)] \notag \\
            &\hat{\mu}(w,x;\mathcal{S},\Pi) \equiv \frac{1}{\# (\{i \in \mathcal{S}_w:X_i\in\mathcal{l}(x;\Pi)\})} \sum_{i \in \mathcal{S}_w:X_i\in\mathcal{l}(x;\Pi)}Y_i^{obs}
\end{align*}

- Average causal effect "in a leaf" and its estimator

\begin{align*}
        &\tau(x;\Pi) \equiv \mathbb{E}[Y_i(1)-Y_i(0)|X_i \in \mathcal{l}(x;\Pi)] \notag \\
        &\hat{\tau}(x;\mathcal{S},\Pi) \equiv \hat{\mu}(1,x;\mathcal{S},\Pi)-\hat{\mu}(0,x;\mathcal{S},\Pi) \notag
\end{align*}


## CART For Heterogeneous Treatment Effects?


::: {.panel-tabset}

### Model
1. Model and Estimation
    - Model type: Tree structure with  $\mathcal{S}^{\color{red}{tr}}$ (Grow and prune) 
    
    - Estimate with $\mathcal{S}^{\color{red}{tr}}$.


$$
            \hat{\tau}(x;\mathcal{S}^{\color{red} \text{tr}},\Pi) = \hat{\mu}(1,x;\mathcal{S}^{\color{red}{tr}},\Pi)-\hat{\mu}(0,x;\mathcal{S}^{\color{red}{tr}},\Pi)
$$


### Criterion
2.  Criterion Function (for fixed complexity parameter $\lambda$)
    - In-sample Goodness-of-fit function: <span style="color: red;">NOT FEASIBLE</span>
    $$
            Q^{is}=-MSE=-\frac{1}{N}\sum_{i=1}^{N}(\tau_i-\hat{\tau}_i)^2
    $$
    - Criterion: $Q^{crit}=Q^{is}-\lambda\cdot\#|\Pi|$
    
### Cross-Validation
3.  Cross-validation approach with $\mathcal{S}^{{\color{red} \text{te}}}$
    - Approach: cv on grid of complexity parameters. Select complexity parameter $\lambda$ with the highest out-of-sample goodness-of-fit ${Q}^{\text{os}}$.
    - ${Q}^{\text{os}}=-MSE$

:::

## Problems with using CART for HTE and its replacement

- GOAL: Estimate within-leaf treatment effect 

- HOW? maximize $-MSE_{\tau}$

<div style="margin-bottom: 0.4em;"></div>

::: {.fragment .custom .fadein-gray}

- Problem 1: $\tau_i$'s are unobservable
    * Under our framework, estimate $-MSE_\tau$ with $-\hat{MSE}_\tau$(unbiased)

<div style="margin-bottom: 0.4em;"></div>

- Problem 2: NOT Honest (e.g.  $\overline{\tau_L}-\overline{\tau_R}$  with condition $\geq c$ is biased)
    * Split sample; one to build tree, the other to estimate effects.

::: {.fragment}

<div style="margin-bottom: 0.4em;"></div>

- NEW criterion

\begin{align*}
        -\mathbb{E}_{\mathcal{S}^{\color{red}{te}}, \mathcal{S}^{\color{red}{est}}}[\sum_{i\in \mathcal{S}^{\color{red}{te}}}(\tau_i-\hat{\tau}(X_i;\mathcal{S}^{\color{red}est}))^2]
\end{align*}

:::

:::

## New Criterion for Honest Causal Tree

Given $\Pi$,

\begin{align*}
        \text{MSE}_{\tau}(\mathcal{S^{{\color{red}\text{te}}}},\mathcal{S^{{\color{red}\text{est}}}}) &\equiv \frac{1}{\text{N}^{\color{red}\text{te}}}\sum_{i\in \mathcal{S^{{\color{red}\text{te}}}}}(\tau_i-\hat{\tau}(X_i;\mathcal{S^{{\color{red}\text{est}}}},\Pi))^2 \notag \\
        &= \frac{1}{\text{N}^{\color{red}\text{te}}}\sum_{i\in \mathcal{S^{{\color{red}\text{te}}}}}(\tau_i^2-2\tau_i\cdot \hat{\tau}(X_i;\mathcal{S^{{\color{red}\text{est}}}},\Pi)+\hat{\tau}^2 (X_i;\mathcal{S^{{\color{red}\text{est}}}},\Pi)) \\
        EMSE &=\mathbb{E}_{\mathcal{S}^{\color{red}{te}}, \mathcal{S}^{\color{red}{est}}}[\sum_{i\in \mathcal{S}^{\color{red}{te}}}(\tau_i-\hat{\tau}(X_i;\mathcal{S}^{\color{red}est}))^2] \\
        &= \mathbb{V}_{\mathcal{S}^{\text{est}}, X_i}[\hat{\tau}(X_i; \mathcal{S}^{\text{est}},\Pi)]-\mathbb{E}_{X_i}[\tau^2(X_i;\Pi)]+\cancel{\mathbb{E}[\tau_i^2]}
\end{align*}


- Again, the last equality holds by the "honesty" that $\mathcal{S}^{est}\perp \Pi$

- Why $\mathbb{V}[\hat{\tau}]$ in $EMSE$? 

    We care not only about this sample but also about other possible samples, so $\mathbb{V}[\hat{\tau}]$ is taken into account.
    
## Estimating the Criterion

- In-sample goodness-of-fit measure $-\hat{EMSE}_{\tau}(\mathcal{S^\text{tr}},\Pi)$
    \begin{align*}
         \equiv 
        \frac{1}{\text{N}^\text{tr}} \sum_{i\in\mathcal{S^\text{tr}}} \hat{\tau}^2(X_i;\mathcal{S}^{tr},\Pi)
        -\biggl(\frac{2}{\text{N}^{\text{tr}}}\biggr)\sum_{\ell \in \Pi}\biggl(\frac{S^2_{\mathcal{S}^\text{tr}_\text{treat}}(\ell)}{p}+\frac{S^2_{\mathcal{S}^\text{tr}_\text{control}}(\ell)}{1-p}\biggr)
\end{align*}
where $p=N^\text{tr}_\text{treat}/N^\text{tr}$

- Note that $S^2$'s are the sample variances of <span style="color: blue;">mean estimates</span>, <span style="color: red;">NOT</span> the treatment effects.
- out-of-sample goodness-of-fit measure: $\hat{EMSE}_\tau(\mathcal{S^{\text{tr, cv}}},\Pi)$


## Interpretation of the Criterion
- The first term <span style="color: blue;">rewards</span> high heterogeneity in treatment effects
    \begin{align*}
        \frac{1}{\text{N}^\text{tr}} \sum_{i\in\mathcal{S^\text{tr}}} \hat{\tau}^2(X_i;\mathcal{S}^{tr},\Pi)
    \end{align*}
- The second term <span style="color: red;">penalizes</span> a partition that increases variance in leaf estimates (e.g. small leaves)
        \begin{align*}
             -\frac{2}{\text{N}^{\text{tr}}}\sum_{\ell \in \Pi}(\frac{S^2_{\mathcal{S}^\text{tr}_\text{treat}}(\ell)}{p}+\frac{S^2_{\mathcal{S}^\text{tr}_\text{control}}(\ell)}{1-p})
        \end{align*}
- Two terms are NOT proportional
    (c.f. Terms in $\hat{EMSE}_\mu$ are proportional)

## Pros and Cons of Honest {#sec-pro-con}
- Pro: 
    * Honest target not only removes potential bias in leaf estimates but also penalizes high variance
    * enables statistical testing(valid confidence intervals)
- Con: smaller sample size, shallower tree, and less personalized predictions

<p align="right">
[<ins>Details</ins>](/#/sec-honest-app)
</p>


# Alternative Estimators and Simulation Results

## Alternative Methods for Constructing Trees

### (1) Fit-based Trees (F)

- Regressors: intercept(average) + <span style="color: red;"> dummy variable for treatment</span>

-  goodness-of-fit 

$$
    MSE_{\mu,W}(\mathcal{S}^{te},\mathcal{S}^{est},\Pi) \equiv
    \sum_{i\in\mathcal{S}^{te}}
    ((Y_i^{\text{obs}}-
    \hat{\mu}_w({\color{red}W_i},X_i;\mathcal{S}^{est},\Pi))^2
    -{Y_i^{\text{obs}}}^2)
$$



- Pros: MSE is feasible (No $\tau_i$ terms)
-  Cons: NO <span style="color: blue;">reward</span> for heterogeneity of treatment effects

    (c.f. $\sum\hat{\tau}^2$ term in Causal Tree MSE)
  
## Alternative Methods for Constructing Trees

### (2) Squared T-statistic Trees (TS)

- Split Rule: Test $H_0$, the <span style="color: red;">CATE</span> is the same in the two potential leaves, with $T^2$

\begin{align*}
    T^2 \equiv N \cdot
    \frac{({\color{red}\overline{\tau}_L-\overline{\tau}_R})^2}{S^2/N_L+S^2/N_R}
\end{align*}
    where $S^2$ is the conditional sample variance given the split

- Pros: (only) rewards for heterogeneity of treatment effects
- Cons: no value on splits that improve the <span style="color: blue;">fit</span>(c.f. **Fit-based Trees**)
    
## Simulation Study: Set-up

 **Goal: Compare the performance of proposed algorithms (Adaptive vs. Honest)**

::: {.panel-tabset}

### Outcome

* Evaluate Mean Squared Error (MSE) for each method
* Evaluate 90% confidence interval coverage for each method

| Notation | Sample Size     | Role                          |
|----------|------------------|-------------------------------|
| N_tr     | 500 or 1,000     | Tree Construction             |
| N_est    | 500 (honest setting) | Treatment Effect Estimation |
| N_te     | 8,000            | Test Sample (MSE eval.)       |



### Model

$$
    Y_i(w) = \eta(X_i) + \frac{1}{2}\cdot (2w-1) \cdot \kappa (X_i) + \epsilon_i
$$
    
$\text{where} \quad \epsilon_i \sim N(0,.01), X_i \sim N(0,1) \quad \text{with} \quad \epsilon_i\perp X_i \quad \text{and }X_i\perp X_j$
     
### Design

We have three different setups:

1. $K=2; \quad \eta(x) = \frac{1}{2}x_1 + x_2; \quad \kappa(x) = \frac{1}{2}x_1$
2. $K=10; \quad \eta(x) = \frac{1}{2}(x_1+x_2) + \sum_{k=3}^6 x_k; \quad \kappa(x) = \frac{2}{1}\mathbb{I}\{x_k >0\}\cdot x_k$
3. $K=20; \quad \eta(x) = \frac{1}{2}\sum_{k=1}^4 x_k + \sum_{k=5}^8 x_k; \quad \kappa(x) = \mathbb{I}\{x_k>0\} \cdot x_k$
<br>
* Design 1: a simple model with two covariates. HTE is linear with $x_1$
* Design 2: six covariates and two that interacts with TE. HTE is non linear 
* Design 3: eight covariates and four that interacts with TE. HTE is non linear 

:::


## Simulation Study: Results
  
### CT-H vs alternative estimators
:::: {.columns}

::: {.column width="40%"}
![](figures/fig2.png)
:::

::: {.column width="60%"}
- CT-H:
    * Best overall performance across all designs

<div style="margin-bottom: 0.5em;"></div>

- F-H:
    * Performs worst in all designs; splits based on outcome prediction

<div style="margin-bottom: 0.5em;"></div>

- TS-H:
    * Strong in Design 1
    * Underperforms in Designs 2 and 3 due to limitations of t-statistics in capturing nonlinear heterogeneity

:::

::::


## Simulation Study: Results{#sec-sim-study}
  
### Adaptive vs Honest : Coverage for 90% confidence intervals

:::: {.columns}

::: {.column width="40%"}
![](figures/fig4.png)
:::

::: {.column width="60%"}
- Honest estimation achieves nominal 90% coverage in all designs, while adaptive methods often fall below

<br>

- The fit estimator has the highest adaptive coverage rates; it doesn’t focus on treatment effects

<br>

- Honest estimation sacrifices some goodness of fit for valid confidence intervals


:::

::::

<p align="right">
[<ins>More results</ins>](/#/sec-sim-app)
</p>

<!-- ## Observational Studies with Unconfoundedness

- Causal Trees can be applied to observational studies under the assumption of unconfoundedness (treatment is independent of potential outcomes given covariates).

<div style="margin-bottom: 0.5em;"></div>

- Since mean differences are biased in observational data, propensity score weighting is used within leaves, with leaf-level normalization to improve efficiency.

<div style="margin-bottom: 0.5em;"></div>

- To enhance robustness, trimming units with extreme propensity scores (near 0 or 1) is recommended.

<div style="margin-bottom: 0.5em;"></div>

- These adjustments can be integrated into the honest estimation phase, and the resulting estimators remain asymptotically normal, allowing for valid inference. -->



## Conclusion
- By having a separate estimation set, tree-based ML approach can be used for estimating and testing heterogeneous treatment effects!

<div style="margin-bottom: 0.5em;"></div>

- It imposes no restrictions on model complexity or the number of covariates, which helps setting data-driven hypotheses.

<div style="margin-bottom: 0.5em;"></div>

- Different criterions can be used(fit, T-squared, etc.), but our baseline estimator(CT-H) performs the best in simulation.

<div style="margin-bottom: 0.5em;"></div>

- Can be extended to observational studies!



## Materials
TBU

# Appendix


## Cost and Benefits of Honest {#sec-honest-app}
- Cost
    * Shallower tree ($\because$ smaller leaves $\rightarrow$ higher $\mathbb{V}$)
    * Smaller # of samples $\rightarrow$ Less personalized predictions and lower MSE
- Benefit
    * EASY
    
    * Holding tree from $\mathcal{S}^{\color{red}tr}$ fixed, can use standard methods to conduct inference (confidence interval) within each leaf of the tree on $\mathcal{S}^{\color{red}te}$
    
    (Disregard of the dimension of covariates)
    * No assumption on sparsity needed (c.f. nonparametric methods)
- vs Dishonest with double the sample
    * Honest does worse if true model is sparse (also the case where bias is less severe)
    * Dishonest has similar or better MSE in many cases, but poor coverage of confidence intervals

<p align="right">
[<ins>Return</ins>](/#/sec-pro-con)
</p>

## FAQ
- Individuals on the edges of a leaf(outliers)
     * Use different method (e.g. Radom Forest) to provide a more personalized estimation. Causal Tree is to answer questions on the relation between covariates and how they interplay with treatment effects.

- Is smaller number of samples bad?
    * Again, we've moved the goal post here. We are not trying to give the best prediction of effect on individuals. Rather, recursive partitioning assists figuring a general relation between covariates and treatment effects.

- Why 50:50 in sample splitting?
    <!-- *Athey initially considered adding a parameter of sample ratio. Imbens thought more than one parameter would be too much to optimize. -->
    * Sample ratio could be taken differently in different problems and data available. 


## Simulation Study: Results{#sec-sim-app}
  
### Number of Leaves(Tree Depth)
:::: {.columns}

::: {.column width="40%"}
![](figures/fig1.png)
:::

::: {.column width="60%"}
- CT-H:
    * Splitting criteria: Maximizes – MSE

<div style="margin-bottom: 0.5em;"></div>

- F-H:
    * Splitting criteria: Maximizes outcome prediction
    * Build deeper trees than that of CT
    * Less prone to overfitting on treatment effects 

<div style="margin-bottom: 0.5em;"></div>

- TS-H:
    * Splitting criteria: Maximizes squared t-statistic
    * Tree depth similar to that of CT
    * Adaptive versions still prone to overfitting
:::

::::

## Simulation Study: Results
  
### Adaptive vs Honest : Ratio of infeasible MSE
:::: {.columns}

::: {.column width="40%"}
![](figures/fig3.png)
:::

::: {.column width="60%"}
- Honest estimation shows higher MSE in most cases
     $\rightarrow$ Uses only half the data, leading to lower precision


<div style="margin-bottom: 0.5em;"></div>

- Fit estimator performs poorly in Design 1
    $\rightarrow$ With smaller sample size, it tends to ignore treatment heterogeneity



<div style="margin-bottom: 0.5em;"></div>

- As design complexity increases, the MSE ratio decreases.
$\rightarrow$ Adaptive estimators overfit more in complex settings.

:::

::::


<p align="right">
[<ins>Return</ins>](/#/sec-sim-study)
</p>
